{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "NLP-1 (Data Cleaning-Vectorization)(ENG)_FC_March24-S.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckFgXU36QC7I"
      },
      "source": [
        "## Install and Import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "lRMDqKMJCRSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVI3B3kcQC7L"
      },
      "source": [
        "# natural language toolkit & contractions\n",
        "!pip install nltk pandas contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKnwZuyRQC7N"
      },
      "source": [
        "ðŸ”‘    :     https://www.nltk.org/api/nltk.tokenize.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stqZ0iNlQC7N"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIVZNeB3QC7O"
      },
      "source": [
        "### Notebook settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JPgI4coQC7O"
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = 'all'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrlwxkdFQC7P"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njmgcSE9QC7P"
      },
      "source": [
        "sample_text= \"\"\"This is pretty cool. A good quality candy might cost $3.88 in New York. But I don't think we buy it. Mr.Biden said 1,000,000$. 2 cars.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHpS0QoqQC7Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwtgntGxQC7Q"
      },
      "source": [
        "#### Sentence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASCpxs_WQQHn"
      },
      "source": [
        "# To use tokenziers\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brw2U9CXQC7Q"
      },
      "source": [
        "sentence_tokens ="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zLpDtxgQC7R"
      },
      "source": [
        "#### WordPunct Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBIVVj60h53r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nks9ILs4QC7R"
      },
      "source": [
        "wordpunc_tokes = \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE3Vs9oTQC7R"
      },
      "source": [
        "#### Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjJ95R43QC7R"
      },
      "source": [
        "word_tokens = \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plKtrYtIQC7S"
      },
      "source": [
        "## Removing Punctuation and Numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOSGvA72QC7S"
      },
      "source": [
        "tokens_without_punc = \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqACGNxcQC7S"
      },
      "source": [
        "## Removing Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J62SbXXRQC7S"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veA2ccj4QC7T"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words(\"english\")\n",
        "print(stop_words)\n",
        "print('len stop_words :', len(stop_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg0P97lTQC7T"
      },
      "source": [
        "words_to_exclude_from_stopwords = ['not', \"n't\", 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", \"don't\", 'hadn', \n",
        "                                   \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", \n",
        "                                   'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \n",
        "                                   \"won't\", 'wouldn', \"wouldn't\"]\n",
        "\n",
        "new_stopwords = [w for w in stop_words if w not in words_to_exclude_from_stopwords]\n",
        "print('len new_stop_words :', len(new_stopwords))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNNf1NVCQC7T"
      },
      "source": [
        "token_without_sw = [t for t in tokens_without_punc if t not in new_stopwords] # \n",
        "print(tokens_without_punc)\n",
        "print(token_without_sw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoKQ-SVtQC7U"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGNumQbXQC7U"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARIUDYl5QC7V"
      },
      "source": [
        "WordNetLemmatizer().lemmatize('driving')\n",
        "WordNetLemmatizer().lemmatize('driver')\n",
        "WordNetLemmatizer().lemmatize('drivers')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AxvJuKBJiim"
      },
      "source": [
        "lem = [WordNetLemmatizer().lemmatize(w) for w in tokens_without_punc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNdV9h47QC7V"
      },
      "source": [
        "print(tokens_without_punc)\n",
        "print(lem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bgiG6R2QC7V"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcXozBsEQC7V"
      },
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJGZVjarlEBf"
      },
      "source": [
        "PorterStemmer().stem(\"driving\")\n",
        "PorterStemmer().stem(\"driver\")\n",
        "PorterStemmer().stem(\"drives\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pwsWQW-QC7V"
      },
      "source": [
        "stem = [PorterStemmer().stem(w) for w in tokens_without_punc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A40zGBvEQC7W"
      },
      "source": [
        "print(tokens_without_punc)\n",
        "print(stem)\n",
        "print(lem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY3VjHdeQC7W"
      },
      "source": [
        "## Joining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fG6UucjQC7W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMaa6hmKuz1U"
      },
      "source": [
        "### Part of Speech Tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEoxQl2FuzCj"
      },
      "source": [
        "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha_24Xn4qvhq"
      },
      "source": [
        "text = \"\"\"Steven Paul Jobs was an American business magnate, industrial designer, investor, and media proprietor. \n",
        "He was the chairman, chief executive officer (CEO), and co-founder of Apple Inc.; the chairman and majority shareholder of Pixar; \n",
        "a member of The Walt Disney Company's board of directors following its acquisition of Pixar; and the founder, chairman, and CEO of NeXT.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcavNQa2mYjU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGLrbD1gmYG0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aZBjiyHu4cd"
      },
      "source": [
        "### NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqkMjoRGu3g5"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6eVhAbHuzRp"
      },
      "source": [
        "from nltk import ne_chunk\n",
        "\n",
        "import matplotlib as mpl\n",
        "import os\n",
        "\n",
        "print(text, '\\n')\n",
        "for chunk in nltk.ne_chunk(pos):\n",
        "      if hasattr(chunk, 'label'):\n",
        "        print(chunk.label(), ' '.join(c[0] for c in chunk))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDyo2N-19Ohz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}