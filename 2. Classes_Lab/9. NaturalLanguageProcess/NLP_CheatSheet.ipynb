{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-CheatSheet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install"
      ],
      "metadata": {
        "id": "gOEgpR0cvaW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!pip install -U spacy-lookups-data\n",
        "!python -m spacy download en_core_web_sm #en_core_web_md #en_core_web_lg\n",
        "\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "pXcHQICkvdKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To use tokenziers\n",
        "nltk.download('punkt')\n",
        "\n",
        "# To use stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# To use Lemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# To use POS\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# To use NER\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# To download all the packages\n",
        "nltk.download()"
      ],
      "metadata": {
        "id": "ofoiqk7IvtrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "G8CB6Ugr2oPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, wordpunct_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "0K5JaMMV2m98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "Ybm_JLR91lNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "tokens = ['compute', 'computer', 'computed', 'computing']\n",
        "for token in tokens:\n",
        "    print(token + ' --> ' + stemmer.stem(token))"
      ],
      "metadata": {
        "id": "11qFN5Wg1pW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "NSb-Pora1tWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "your_text = \"Your text goes here\"\n",
        "[WordNetLemmatizer().lemmatize(token) for token in your_text.split()]\n",
        "\n",
        "# Spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Was Google founded in early 1990?\")\n",
        "[(x.orth_, x.lemma_) for x in [token for token in doc]]"
      ],
      "metadata": {
        "id": "uzuBWxBi1yHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Cleaning Function"
      ],
      "metadata": {
        "id": "NaYSpvfYxQgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_cleaning(data):\n",
        "\n",
        "    # Discalimer: This is a sample text cleaning function. You might need to modify it\n",
        "    \n",
        "    import re\n",
        "    \n",
        "    #1. Removing URLS\n",
        "    data = re.sub(r'http\\S+', '', data)\n",
        "\n",
        "    #2. Removing Tags\n",
        "    data = re.sub(r'#\\w+', '', data)\n",
        "\n",
        "    #3. Removing Mentions\n",
        "    data = re.sub(r'@\\w+', '', data)\n",
        "\n",
        "    #4. Contractions Expension & Tokenize\n",
        "    #text_tokens = word_tokenize(contractions.fix(data.lower())) \n",
        "    text_tokens = word_tokenize(data.replace(\"'\", '').lower())\n",
        "\n",
        "    #5. Removing mentions\n",
        "    tokens_without_mention = [w for w in text_tokens if not w.startswith('@')]\n",
        "    \n",
        "    #6. Remove Puncs\n",
        "    tokens_without_punc = [w for w in tokens_without_mention if w.isalpha()]\n",
        "    \n",
        "    #7. Removing Stopwords\n",
        "    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n",
        "    \n",
        "    #8. lemma\n",
        "    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n",
        "    \n",
        "    #joining\n",
        "    return \" \".join(text_cleaned)"
      ],
      "metadata": {
        "id": "BqGdx9JmxPl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train-Test Split"
      ],
      "metadata": {
        "id": "mIxzDJMryL7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X --> your text column in your data frame\n",
        "# y --> your target column in your data frame"
      ],
      "metadata": {
        "id": "ORRmdamqyd2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 4299)"
      ],
      "metadata": {
        "id": "v5llYqTwyLQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF Vectorizer"
      ],
      "metadata": {
        "id": "OTMyk92nwt3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Resource:*** https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer"
      ],
      "metadata": {
        "id": "ia8tSMtgxtww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "PdZRIsYowODU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: text_cleaning is your custom cleaning function\n",
        "tf_idf_vectorizer = TfidfVectorizer(preprocessor=text_cleaning, min_df=2, ngram_range=(1,2))\n",
        "X_train_tf_idf = tf_idf_vectorizer.fit_transform(X_train['clean_text'])\n",
        "X_test_tf_idf = tf_idf_vectorizer.transform(X_test['clean_text'])"
      ],
      "metadata": {
        "id": "NHA13Il0wsKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CountVectorizer"
      ],
      "metadata": {
        "id": "fgKA73mYy0px"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Resource:*** https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer"
      ],
      "metadata": {
        "id": "f02TvDMey5Js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "cNq72WwF0IjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(preprocessor=text_cleaning, min_df=2, ngram_range=(1,2))\n",
        "X_train_count = vectorizer.fit_transform(X_train['text'])\n",
        "X_test_count = vectorizer.transform(X_test['text'])"
      ],
      "metadata": {
        "id": "-hhcrW8myzmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy"
      ],
      "metadata": {
        "id": "3W5AjA__0XnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp('Your text goes here')\n",
        "for token in doc:\n",
        "    print(f\"\"\"Token -> {token.text:{8}} Lemma -> {token.lemma_:{8}} POS -> {token.pos_:{8}} TAG -> {token.tag_:{8}} Shape -> {token.shape_:{8}} Is_Alpha -> {token.is_alpha:{2}}\"\"\")"
      ],
      "metadata": {
        "id": "G0FhWUBN0dQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NER"
      ],
      "metadata": {
        "id": "W658veAY28bL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entities supported by spacy:\n",
        "\n",
        "**PERSON** People, including fictional.\n",
        "\n",
        "**NORP** Nationalities or religious or political groups.\n",
        "\n",
        "**FAC** Buildings, airports, highways, bridges, etc.\n",
        "\n",
        "**ORG** Companies, agencies, institutions, etc.\n",
        "\n",
        "**GPE** Countries, cities, states.\n",
        "\n",
        "**LOC** Non-GPE locations, mountain ranges, bodies of water.\n",
        "\n",
        "**PRODUCT** Objects, vehicles, foods, etc. (Not services.)\n",
        "\n",
        "**EVENT** Named hurricanes, battles, wars, sports events, etc.\n",
        "\n",
        "**WORK_OF_ART** Titles of books, songs, etc.\n",
        "\n",
        "**LAW** Named documents made into laws.\n",
        "\n",
        "**LANGUAGE** Any named language.\n",
        "\n",
        "**DATE** Absolute or relative dates or periods.\n",
        "\n",
        "**TIME** Times smaller than a day.\n",
        "\n",
        "**PERCENT** Percentage, including ”%“.\n",
        "\n",
        "**MONEY** Monetary values, including unit.\n",
        "\n",
        "**QUANTITY** Measurements, as of weight or distance.\n",
        "\n",
        "**ORDINAL** “first”, “second”, etc.\n",
        "\n",
        "**CARDINAL** Numerals that do not fall under another type."
      ],
      "metadata": {
        "id": "_0UW7aMp2_XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "    print(f'{ent.text:{15}} --> {ent.label_}')"
      ],
      "metadata": {
        "id": "tZ7IPp9h0rrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spacy Visualization"
      ],
      "metadata": {
        "id": "r0HPZaxu0xoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "doc = nlp('Apple is looking for buying a UK startup for $1 billion in 2020')\n",
        "displacy.render(doc, style = 'ent', jupyter=True, options={'distance': 90})"
      ],
      "metadata": {
        "id": "Meftddhu0w4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spacy Sentence Segmentation"
      ],
      "metadata": {
        "id": "eiFxixs-1Azy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Apple is looking for buying a U.K. startup. Government has given permission.'\n",
        "doc = nlp(text)\n",
        "\n",
        "for sent in doc.sents:\n",
        "    print(sent)"
      ],
      "metadata": {
        "id": "apurVO0l0742"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similarity"
      ],
      "metadata": {
        "id": "1Fg-ijMU34LB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To use similarity, spacy model should be at least en_core_web_md\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "JKWJOrVw329s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare 2 docs\n",
        "doc1 = nlp('I like fast food')\n",
        "doc2 = nlp('I like pizza')\n",
        "\n",
        "print(doc1.similarity(doc2))"
      ],
      "metadata": {
        "id": "x93v0aZS4MAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare tokens\n",
        "\n",
        "doc = nlp('I like pizza and pasta')\n",
        "token1 = doc[2]\n",
        "token2 = doc[4]\n",
        "\n",
        "print(token1.similarity(token2))"
      ],
      "metadata": {
        "id": "64ABMoWw4O0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting vector\n",
        "\n",
        "doc[2].vector"
      ],
      "metadata": {
        "id": "yaxPH6eJ4lYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spell Checker"
      ],
      "metadata": {
        "id": "-RwDcvvk4z3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob\n",
        "!pip install autocorrect\n",
        "!pip install pyspellchecker"
      ],
      "metadata": {
        "id": "vLqWRiRN4zPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spellchecker import SpellChecker\n",
        "spell = SpellChecker()\n",
        "docs = ['calandar', 'lighteinig', 'misspel', 'booq', 'undrstand', 'receive', 'adress']\n",
        "\n",
        "# Correction\n",
        "for word in docs:\n",
        "    print(f'{word:{10}} --> {spell.correction(word):{20}}')\n",
        "\n",
        "# Candidates\n",
        "for word in docs:\n",
        "    print(f'{word:{10}} --> {spell.candidates(word)}')"
      ],
      "metadata": {
        "id": "fcU8Mkyh5D1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEXTBLOB\n",
        "\n",
        "from textblob import TextBlob, Word\n",
        "\n",
        "ex = TextBlob('He was veri happy in hisn neww locotion.')\n",
        "\n",
        "for word in ex.words:\n",
        "    print(word, \":\", word.correct())"
      ],
      "metadata": {
        "id": "0eNBJhtx5HK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "-ZF6KqAU5kvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testimonial = TextBlob(\"The food was great!\")\n",
        "print(testimonial.sentiment)"
      ],
      "metadata": {
        "id": "znk3UXR15nzi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}